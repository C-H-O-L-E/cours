{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Notes de cours - MEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commandes sur jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nouvelle cellule en dessous: B\n",
    "- Nouvelle cellule au dessus: A\n",
    "- Supprimer cellule : dd\n",
    "- Passer en mode edition : Echap\n",
    "- Passer en mode insertion : Entrée\n",
    "- Executer la cellule : Shift+Entree\n",
    "- Transformer une cellule de code en cellule de texte : m\n",
    "- Repasser en mode code : y\n",
    "- Faire un titre dans une cellule de texte : #Titre - ## soustitre - - - ###Sous-soustitre\n",
    "- Pour enregistrer : Ctrl + S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions diverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un random choice dans une liste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda x : True if (x > 10 and x < 20) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lister les fichiers dans un dossier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "liste_fichiers_close=os.listdir('close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations sur les strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('tableacharger.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_cleaned.pkl\", \"rb\") as f:\n",
    "    data_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chatbot_database.txt', \"r\") as fichier:\n",
    "        file=fichier.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(n=10)\n",
    "df.shape # donne le nbre de lignes et colonnes\n",
    "df.info() # donne la structure du tableau\n",
    "df.describe() # va sortir toutes les stats sur les colonnes de type ‘int’ ou ‘float’\n",
    "df.describe(include= « object ») # va sortir toutes les stats sur les colonnes de type « string »\n",
    "df.describe(include= « all ») # va sortir toutes les stats\n",
    "df[« duration »].max()\n",
    "Df[« genre »].nunique() # nombre de genres uniques\n",
    "Df[« genre »].unique() # liste des genres uniques\n",
    "df[« genre »].value_counts() # compte l’occurrence de chacune des valeurs uniques\n",
    "df[« genre »].value_counts(normalize=True) # compte l’occurrence de chacune des valeurs uniques en % \n",
    "df[‘genre’].values # retourne un array (Panda est une surcouche de numpy)\n",
    "df[‘genre’].index # retourne un array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supression des Nan :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna.sum() \n",
    "df= df.drop(‘Cabin’, axis=1) #Suppression d'une colonne\n",
    "df=df.dropna(subset=[« Embarked »]) #Suppression des lignes avec des Nan \n",
    "df=df.fillna(mean_age) #Remplacer valeurs nulles par moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des duplicats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df.duplicated -> liste de bouleens\n",
    "Df.duplicated.count()\n",
    "Df.drop_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplacement des données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un Imputer va remplacer tous les Nan du dataset\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imp.fit_transform(imputer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "KNN_imputer = KNNImputer(n_neighbors=10)\n",
    "X_KNN = KNN_imputer.fit_transform(imputer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement des données categorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, pd.get_dummies(data['gender'],prefix='Gender', drop_first=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaliser la donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_continuous)\n",
    "X_scaled_mm = scaler.transform(X_continuous)\n",
    "X_scaled_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_continuous)\n",
    "X_scaled_sc = scaler.transform(X_continuous)\n",
    "X_scaled_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_continuous)\n",
    "X_scaled_rs = scaler.transform(X_continuous)\n",
    "X_scaled_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "scaler = PowerTransformer(method='yeo-johnson')\n",
    "scaler.fit(X_continuous)\n",
    "X_scaled_ps = scaler.transform(X_continuous)\n",
    "X_scaled_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réduire ou Completer la donnée pour des classes sous représentées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "data=resample(data, n_samples=10000,  stratify=data['target'],random_state=0,replace=False)\n",
    "#replace = false permet d'eviter d'avoir des doublons, le stratify permet de garder la meme proportion de labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_train_resampled, y_train_resampled = RandomUnderSampler(sampling_strategy=0.1).fit_resample(X_train, y_train)\n",
    "#on peut faire sampling_strategy =0.1 pour imposer la proposortion de la classe minimal à 10%\n",
    "#ici on ecarte une partie du dataset pour reequilibrer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train) \n",
    "#on peut faire sampling_strategy =0.2 pour imposer la proposortion de la classe minimal à 20%\n",
    "#ici on génère des nouveaux points à partir des points existants\n",
    "#pour la detection d'anomalie, on peut faire du SMOTE pour du supervisé mais pas pour du non supervisé "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporter la donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_cleaned.pkl', 'wb') as f:\n",
    "    pickle.dump(data_cleaned, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sns.boxplot(‘Fare’,data=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sns.countplot(x = ”sex”, data =df, palette =’Blues’)\n",
    "\n",
    "for label,value in enumerate(nb_sexes):\n",
    "    plt.text(x=label, y=value-50, s= value, ha=’center’)\n",
    "Sns.barblot(x=survivalrate.index, y=survivalrate.value, palette =’Blues’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sns.distplot(df[‘age’]) #histogramme des ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "ax = sns.heatmap(corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des tous les scatterplots --> visualisation des outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition du set de test et d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) \n",
    "# mettre shuffle = False pour les times series et garder des blocs temporels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3) \n",
    "knn.fit(X=X, y=y)\n",
    "y_pred = knn.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X=X, y=y)\n",
    "y_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes --> family of simple \"probabilistic classifiers\" based on Bayes' theorem. It implies a strong (this is why this algorithm is considered naive) independence assumptions between the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "y_pred = gnb.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support-vector machine (SVM)\n",
    "--> constructs what is called a hyperplane (a separation - such a line in 2D space, or a surface plane in 3D) to separate classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X, y)\n",
    "y_pred = svc.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM avec le \"Kernel-trick\" --> permet de trouver une transformation qui sépare au mieux les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_k = SVC(kernel=\"rbf\")\n",
    "svc_k.fit(X, y)\n",
    "y_pred_k = svc_k.predict(X)\n",
    "acc_k = accuracy_score(y, y_pred_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoostClassifierm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "ada.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "GBC.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "\n",
    "# Instantiate the object\n",
    "xg = xgb.XGBClassifier()\n",
    "\n",
    "# Fit on the data\n",
    "xg.fit(X_train, y_train)\n",
    "y_pred_xg = xg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lg = lgb.LGBMClassifier(silent=False)\n",
    "lg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "\n",
    "cb = cb.CatBoostClassifier(iterations=20, loss_function='MultiClass')\n",
    "cb.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest :\n",
    "- is a Bagging algorithm\n",
    "- that makes a forest of decision trees\n",
    "- gives the same weight to each decision tree\n",
    "- grows trees independently\n",
    "\n",
    "Adaboost :\n",
    "- is a Boosting algorithm\n",
    "- combines weak learners (most often stumps)\n",
    "- gives a different weight to each stump depending on their performance\n",
    "- each stump depends on the mistakes of the previous ones\n",
    "\n",
    "Gradient Boosting Regression :\n",
    "- is a Boosting algorithm\n",
    "- combines trees\n",
    "- computes the residuals of the previous step\n",
    "- attaches a learning rate to each model\n",
    "\n",
    "Gradient Boosting Classification :\n",
    "- same as regression\n",
    "- applies as transformation to allow for classification\n",
    "\n",
    "XGBoost :\n",
    "- super fast implementation\n",
    "- Histogram-based split to build trees\n",
    "\n",
    "LGBM :\n",
    "- developed by Microsoft\n",
    "- leaf-wise development depending of gradient\n",
    "\n",
    "Catboost :\n",
    "- developed by Yandex\n",
    "- supports categorical features\n",
    "- supports GPU natively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesure de la performance d'un modèle de classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy donne le % de prédiction égale à la valeur attenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper paramètres et over/under-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour verifier qu'un modèle ne fait pas d'over/under fitting, il peut etre interressant de diviser le jeu d'entrainement en plusieurs jeux : test/validation. On compare alors les resultat du modele sur ces jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# The KFold object returns arrays of indices, not of values!\n",
    "for train, val in kfold.split(data, y):\n",
    "    # Creating the train set and validation set\n",
    "    X_train = data[train].reshape(-1, 1)\n",
    "    y_train = y[train]\n",
    "    X_val = data[val].reshape(-1, 1)\n",
    "    y_val = y[val]\n",
    "    \n",
    "    # Fitting the model on the data\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_val)\n",
    "    \n",
    "    # Getting the model's score on each fold\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    scores.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir le score sur tous les jeux de validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator=SVC(), X=X_train, y=y_train, cv=5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcourir des hyper-paramètres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [1, 10, 100], \n",
    "              'gamma': [0.001, 0.01, 0.1]}\n",
    "#ou:\n",
    "param_grid = dict(C=uniform(loc=0, scale=4),  penalty=[‘l2’, ‘l1’])\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                   param_grid,\n",
    "                   cv=3, # In order to test the different hyperparameters (on the train set), \n",
    "                         # we use the `cross validation` technique.\n",
    "                         # 3 represents the number of folds of the cross-val.\n",
    "                   verbose=1,  # Setting Verbose adds some \"prints\" (logs) detailing\n",
    "                   n_jobs=-1    # what is happening in backend\n",
    "                                # The higher the setting, the higher the nb of logs printed\n",
    "                  )\n",
    "#pour faire appel aux resultats:\n",
    "grid.fit(X_train, y_train)\n",
    "grid.cv_results_\n",
    "pd.DataFrame(grid.cv_results_)\n",
    "grid.best_params_\n",
    "grid.cv_results_[\"mean_test_score\"]\n",
    "grid.best_score_\n",
    "grid.best_estimator_ # retourne le meilleur model\n",
    "grid.param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode similaire avec RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "param_dist = {\n",
    "    'C': scipy.stats.expon(scale=100),\n",
    "    'gamma': scipy.stats.expon(scale=.1)\n",
    "    }\n",
    "\n",
    "n_iter_search = 50 # n_iter is the number of hyperparameters settings that are tried\n",
    "grid = RandomizedSearchCV(clf,\n",
    "                         param_distributions=param_dist,\n",
    "                         n_iter=n_iter_search,\n",
    "                         verbose=1,\n",
    "                         cv=5, \n",
    "                         n_jobs=-1) # pour dire qu'on utilise tous les coeurs du processeur\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.cv_results_\n",
    "pd.DataFrame(grid.cv_results_)\n",
    "grid.best_params_\n",
    "grid.cv_results_[\"mean_test_score\"]\n",
    "grid.best_score_\n",
    "grid.best_estimator_ # retourne le meilleur model\n",
    "grid.param_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "Knr = KNeighborsRegressor(n_neighbors=3)\n",
    "Knr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine Regressor :\n",
    "The target of the algorithm is to find a line or a curve (depending on the used kernel) that contains the maximum number of points within a margin interval and that minimizes the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_linear = SVR(kernel=\"linear\")\n",
    "svr_linear.fit(X_train,y_train)\n",
    "y_pred = svr_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANSAC allows you to remove outliers and make a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "ransac = RANSACRegressor()\n",
    "ransac.fit(X_train,y_train)\n",
    "y_pred = ransac.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesure de la performance d'un modèle de regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = mean_squared_error(y, y_pred,squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction du nombre de dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode PCA : Princpal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=2) # on peut aussi mettre n_components=0.99 pour definir la target de explained variance ratio\n",
    "# first we perform mean normalization\n",
    "X_centered = X - X.mean(axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage non supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection des anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Outlier Factor (LOF) --> calcule la distance moyenne d'un point parrapport à ses voisins et la compare à la moyenne des distances moyennes des voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=\"auto\")\n",
    "# use fit_predict to compute the predicted labels of the training samples\n",
    "# (when LOF is used for outlier detection, the estimator has no predict,\n",
    "# decision_function and score_samples methods).\n",
    "y_pred = lof.fit_predict(X_train_resampled)\n",
    "n_errors = (y_pred != y_train_resampled).sum()\n",
    "X_scores = lof.negative_outlier_factor_ #donne un facteur d'eloignement du point par rapport aux autres\n",
    "print(\"prediction errors: {}\".format(n_errors))\n",
    "print(\"Offset (threshold to consider sample as anomaly or not): {}\".format(-lof.offset_))\n",
    "print(\"LOF scores: {}\".format(-lof.negative_outlier_factor_))\n",
    "\n",
    "print(classification_report(y_pred,y_train_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "IF = IsolationForest(n_estimators=200, \n",
    "                              max_samples=200, \n",
    "                              contamination='auto', \n",
    "                              n_jobs=-1, \n",
    "                              behaviour='deprecated', \n",
    "                              random_state=42) #contamination: amount of outliers to determine threshold value for outlier detection ; behaviour new is to match scikit-learn decision_function method in other anomaly detection algorithms \n",
    "\n",
    "IF.fit(X_train_resampled1)\n",
    "ypredIF = IF.predict(X_test)\n",
    "\n",
    "# Warning, those algorithms return -1 for anomaly and 1 for normal!!\n",
    "ypredIF[ypredIF == 1] = 0\n",
    "ypredIF[ypredIF == -1] = 1\n",
    "\n",
    "print(classification_report(y_test_conv, ypredIF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing : tokenizen, supprimer ponctuation, supprimer les stop words, prendre les racines des mots.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data['Texte_tokens']=data['Texte'].apply(lambda x: word_tokenize(x))\n",
    "data['Texte_tokens_nopunc']=data['Texte_tokens'].apply(lambda x: [t for t in x if t.isalpha()])\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "data['Texte_tokens_cleaned']=data['Texte_tokens_nopunc'].apply(lambda x: [t for t in x if t not in stop_words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le data processing peut aussi se faire par operations sur les arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=np.array(word_tokenize(x.lower()))\n",
    "token=token[np.char.isalpha(token)]\n",
    "stop_words=np.array(stopwords.words(\"english\"))\n",
    "token = token[np.isin(token,stop_words,invert=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, wordpunct_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    output = np.asarray(pos_tag)\n",
    "    for i in range(len(pos_tag)):\n",
    "        if pos_tag[i][1].startswith('J'):\n",
    "            output[i][1] = wordnet.ADJ\n",
    "        elif pos_tag[i][1].startswith('V'):\n",
    "            output[i][1] = wordnet.VERB\n",
    "        elif pos_tag[i][1].startswith('R'):\n",
    "            output[i][1] = wordnet.ADV\n",
    "        else:\n",
    "            output[i][1] = wordnet.NOUN\n",
    "    return output\n",
    "\n",
    "\n",
    "def preprocess_nltk(quote,StemOrLem):\n",
    "    quote = quote.lower()\n",
    "    tokens = word_tokenize(quote)\n",
    "    token_punc = [t for t in tokens if t.isalpha()]\n",
    "    token_stop = [t for t in token_punc if t not in stop_words]\n",
    "    \n",
    "    if StemOrLem.lower()==\"stemmer\":\n",
    "        token_stemmed=[stemmer.stem(t) for t in token_stop]\n",
    "    else:\n",
    "        tokens_pos_tag=nltk.pos_tag(token_stop)\n",
    "        tokens_pos_tag_2 = get_wordnet_pos(tokens_pos_tag)\n",
    "        token_stemmed = [lemmatizer.lemmatize(w,t) for w,t in tokens_pos_tag_2]\n",
    "\n",
    "    return token_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prendre les racines des mots : stemmer ou lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "data['Texte_stemmed']=data['Texte_tokens_cleaned'].apply(lambda x: [stemmer.stem(t) for t in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le lemmatizer il faut renseigner le type des mot, il faut utiliser une table de correspondant pour traduire le type des mots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtenir le type des mots\n",
    "tokens_pos_tag=nltk.pos_tag(tokens_no_stops)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Traduire le type des mots\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    output = np.asarray(pos_tag)\n",
    "    for i in range(len(pos_tag)):\n",
    "        if pos_tag[i][1].startswith('J'):\n",
    "            output[i][1] = wordnet.ADJ\n",
    "        elif pos_tag[i][1].startswith('V'):\n",
    "            output[i][1] = wordnet.VERB\n",
    "        elif pos_tag[i][1].startswith('R'):\n",
    "            output[i][1] = wordnet.ADV\n",
    "        else:\n",
    "            output[i][1] = wordnet.NOUN\n",
    "    return output\n",
    "\n",
    "tokens_pos_tag_2 = get_wordnet_pos(tokens_pos_tag)\n",
    "\n",
    "#executer le lemmetizer\n",
    "lem_words = [lemmatizer.lemmatize(w,t) for w,t in tokens_pos_tag_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rechercher les similitudes entre un texte et une liste de textes on vectorise le texte. Deux méthodes:\n",
    "- On vectorise en comptant l'occurence des mots : BOW\n",
    "- On vectorise en comptant la presence ou non des mots et on met en exergue les mots qui sont rares et donc differenciant : TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOWvectorizer = CountVectorizer( stop_words='english')\n",
    "BOW = BOWvectorizer.fit_transform(data[\"full\"]).toarray()\n",
    "bow_df = pd.DataFrame(data=BOW, columns=BOWvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "TFIDFvectorizer = TfidfVectorizer( stop_words='english',lowercase = False, analyzer=lambda x: x)# lambda x:x pour lui dire de ne rien faire pour preprocessé car on l'a deja fait\n",
    "tf_idf = TFIDFvectorizer.fit_transform(data[\"full\"]).toarray()\n",
    "TFIDF=pd.DataFrame(data=tf_idf, columns=TFIDFvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesure de la similarité : deux mesures :\n",
    "- Jaccard similarity : J=intersection/union\n",
    "- Cosine similarity : Produit scalaire de deux vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_A = set(A.split()) \n",
    "tokens_B = set(B.split())\n",
    "\n",
    "# Compute the intersection and union\n",
    "intersection = tokens_A.intersection(tokens_B)\n",
    "\n",
    "union = tokens_A.union(tokens_B)\n",
    "# Compute and print the Jaccard Similarity\n",
    "J = len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "print(pairwise.cosine_similarity(TFIDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK / SPARK ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouvrir un session spark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/chloe/programs/spark-3.0.0-preview2-bin-hadoop2.7/')\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"ForestCover\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OU :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession.builder.appName(\"exercice4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger une base de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../../../../../../data/forest/train-set.csv\", \\\n",
    "                     header=True, \\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille du dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataframe\n",
    "print((on_time_dataframe.count(), len(on_time_dataframe.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commandes spark diverses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(2,truncate= True)\n",
    "df.count()\n",
    "len(train.columns), train.columns\n",
    "train.describe().show()\n",
    "f.select('Total Dispatched Trips','Unique Dispatched Vehicle').describe().show()\n",
    "df.groupby('ESPECE').count().orderBy('count',ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifier type de donnes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"goal\", df.goal.cast('int')) \\\n",
    "    .withColumn(\"deadline\", df.deadline.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df=df.withColumn(\"state_changed_at\", F.to_timestamp(df.state_changed_at)) #1er argument donne le nom de la nouvelle colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser des fonctions de transformation de données personnalisées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "udfCountry = F.udf(lambda country, currency : currency if country==False else country, IntegerType()) # dernier argument donne le type de données de sortie.\n",
    "df=df.withColumn('country2', udfCountry(df.country,df.currency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BinaryType – Données binaires.\n",
    "- BooleanType – Valeurs booléennes.\n",
    "- ByteType – Valeur d'octet.\n",
    "- DateType – Valeur d'horodatage.\n",
    "- DoubleType – Valeur double à virgule flottante.\n",
    "- IntegerType – Valeur d'entier.\n",
    "- LongType – Valeur d'entier long.\n",
    "- NullType – Valeur null.\n",
    "- ShortType – Valeur d'entier court.\n",
    "- StringType – Chaîne de texte.\n",
    "- TimestampType – Valeur d'horodatage (généralement en secondes à partir du 01/01/1970).\n",
    "- UnknownType – Valeur de type non identifié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer en DataFrame pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pandas = trimmed_cast_performance.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a vector representation for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['shots', 'hits', 'assists','penaltyMinutes','timeOnIce','takeaways'],\n",
    "                            outputCol=\"features\")\n",
    "train_df = assembler.transform(df)\n",
    "train_df.select('features').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split avec spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression lineaire sur spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='goals')\n",
    "lr_model = lr.fit(train_df)\n",
    "# Output statistics \n",
    "trainingSummary = lr_model.summary\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de pipe avec scale / kmeans :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "inputcol = df.drop('Class').columns #on enleve la colonne du target car non supervisé ici\n",
    "assembler = VectorAssembler(inputCols = inputcol, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "kmeans = KMeans(featuresCol=\"scaledfeatures\").setK(2).setSeed(1)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "km_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice de correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "corr_mat = Correlation.corr(features_vec, \"features2\").head()[0]\n",
    "corr_array = corr_mat.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour voir si une serie est dtationnaire : test de Dickey-Fuller : la valeur du ADF doit etre inferieure à la marge d'erreur 5% ou 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(df['value'].ffill(0))\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importer / Lire une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "k = Image.open(\"/content/DSCF8579.JPG\")\n",
    "k = k.resize((int(k.size[0]/10), int(k.size[1]/10)))\n",
    "k.save(\"image1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('images/Mark.jpg')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter un drop out permet de limiter la dépendence envers certains noeuds d'une couche et limite le surapprentisage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection de visage / d'oeil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adresse des fichiers : https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection des visages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate and get the path of the cascade classifier for face detection\n",
    "FACE_CASCADE_PATH = \"/content/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# Instantiate a face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(FACE_CASCADE_PATH)\n",
    "\n",
    "# Detect faces\n",
    "faces = face_cascade.detectMultiScale(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracer des rectangles sur l'image d'origine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each found face\n",
    "for (x, y, w, h) in faces: \n",
    "    # Draw a box around the faces\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
    "    \n",
    "# Display the result\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détection des yeux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate and get the path of the cascade classifier for face detection\n",
    "EYES_CASCADE_PATH = \"/content/haarcascade_eye.xml\"\n",
    "\n",
    "# Instantiate a face cascade classifier\n",
    "eyes_cascade = cv2.CascadeClassifier(EYES_CASCADE_PATH)\n",
    "\n",
    "# Detect faces\n",
    "eyes = eyes_cascade.detectMultiScale(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "                                 value=0,\n",
    "                                 padding='post', # to add zeros at the end\n",
    "                                 truncating='post', # to cut the end of long sequences\n",
    "                                 maxlen=128) # the length we want\n",
    "\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "                                value=0,\n",
    "                                padding='post', # to add zeros at the end\n",
    "                                truncating='post', # to cut the end of long sequences\n",
    "                                maxlen=128) # the length we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'un RNN avec un vecteur en entrée (type phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "\n",
    "def my_RNN():\n",
    "\n",
    "    model = Sequential()\n",
    "    # The input_dim is the number of different words we have in our corpus: here 10000\n",
    "    # The input_length is the length of our sequences: here 128 thanks to padding\n",
    "    model.add(Embedding(input_dim=10000, output_dim=32, input_length=128))\n",
    "\n",
    "    # We add two layers of RNN \n",
    "    model.add(SimpleRNN(units=32, return_sequences=True))\n",
    "    model.add(SimpleRNN(units=32, return_sequences=False))\n",
    "    \n",
    "    # Finally we add a sigmoid\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = my_RNN()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'un RNN avec en entrée une serie temporelle donc une matrice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "\n",
    "def RNN():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=32, input_shape=(X_train.shape[1],X_train.shape[2]), return_sequences=True))\n",
    "    #ici on n'a pas input_dim mais input_shape!\n",
    "    #la premiere couche doit renvoyer un return_sequence=True pour prendre en compte l'historique\n",
    "    model.add(SimpleRNN(units=16, return_sequences=False))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = RNN()\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "\n",
    "model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "\n",
    "y_pred=model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
